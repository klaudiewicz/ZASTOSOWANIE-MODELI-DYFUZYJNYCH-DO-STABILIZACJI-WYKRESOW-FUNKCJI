{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a990ba44-6236-4b3e-9f6f-71bbde5bcdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Używane urządzenie: {DEVICE}\")\n",
    "\n",
    "# Parametry\n",
    "N_SAMPLES = 1000\n",
    "SEQ_LENGTH = 128\n",
    "X_MIN, X_MAX = -5, 5\n",
    "T_STEPS = 100         \n",
    "BETA_START = 0.0001\n",
    "BETA_END = 0.02\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-3\n",
    "EPOCHS = 2000             \n",
    "\n",
    "# ==========================================\n",
    "# 1. PRZYGOTOWANIE DANYCH\n",
    "# ==========================================\n",
    "\n",
    "def generate_random_curves(n_samples, seq_length):\n",
    "    data = []\n",
    "    x = np.linspace(X_MIN, X_MAX, seq_length)\n",
    "    for _ in range(n_samples):\n",
    "        freq1 = np.random.uniform(0.5, 3.0)\n",
    "        freq2 = np.random.uniform(0.5, 3.0)\n",
    "        phase = np.random.uniform(0, 2*np.pi)\n",
    "        amp1 = np.random.uniform(0.5, 1.5)\n",
    "        amp2 = np.random.uniform(0.1, 0.5)\n",
    "        trend = np.random.uniform(-0.1, 0.1) * x\n",
    "        y = amp1 * np.sin(freq1 * x + phase) + amp2 * np.cos(freq2 * x) + trend\n",
    "        # Normalizacja\n",
    "        y = (y - y.mean()) / (y.std() + 1e-8)\n",
    "        data.append(y)\n",
    "    return np.array(data, dtype=np.float32), x\n",
    "\n",
    "curves_data, x_axis = generate_random_curves(N_SAMPLES, SEQ_LENGTH)\n",
    "\n",
    "class SignalDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.from_numpy(data).unsqueeze(1)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "dataloader = DataLoader(SignalDataset(curves_data), batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Harmonogram Dyfuzji\n",
    "betas = torch.linspace(BETA_START, BETA_END, T_STEPS).to(DEVICE)\n",
    "alphas = 1.0 - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0).to(DEVICE)\n",
    "\n",
    "# # ==========================================\n",
    "# # 2. MODELE \n",
    "# # ==========================================\n",
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    def forward(self, t):\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = t[:, None].float() * emb[None, :]\n",
    "        emb = torch.cat((torch.sin(emb), torch.cos(emb)), dim=1)\n",
    "        return emb\n",
    "\n",
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self, t_emb_dim=32):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPosEmb(t_emb_dim),\n",
    "            nn.Linear(t_emb_dim, t_emb_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.time_emb_proj = nn.Linear(t_emb_dim, 64)\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(1, 32, 5, padding=2)\n",
    "        self.conv2 = nn.Conv1d(32, 64, 5, padding=2)\n",
    "        self.conv3 = nn.Conv1d(64, 32, 5, padding=2)\n",
    "        self.conv4 = nn.Conv1d(32, 1, 5, padding=2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_emb = self.time_mlp(t)\n",
    "        t_emb = self.time_emb_proj(t_emb)[:, :, None]\n",
    "        \n",
    "        h = self.relu(self.conv1(x))\n",
    "        h = self.relu(self.conv2(h))\n",
    "        h = h + t_emb \n",
    "        h = self.relu(self.conv3(h))\n",
    "        h = self.conv4(h)\n",
    "        return h\n",
    "\n",
    "# Denoising Autoencoder (DAE) ---\n",
    "class DenoisingAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv1d(64, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv1d(32, 1, 3, padding=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# ==========================================\n",
    "# 3. FUNKCJE SAMPLOWANIA\n",
    "# ==========================================\n",
    "\n",
    "def forward_diffusion_sample(x0, t):\n",
    "    sqrt_alpha = torch.sqrt(alphas_cumprod[t]).view(-1, 1, 1)\n",
    "    sqrt_one_minus = torch.sqrt(1 - alphas_cumprod[t]).view(-1, 1, 1)\n",
    "    noise = torch.randn_like(x0)\n",
    "    return sqrt_alpha * x0 + sqrt_one_minus * noise, noise\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_reverse_process(model, x_noisy):\n",
    "    model.eval()\n",
    "    x = x_noisy.clone()\n",
    "    for t_inv in reversed(range(T_STEPS)):\n",
    "        t = torch.tensor([t_inv], device=DEVICE).expand(x.shape[0])\n",
    "        pred_noise = model(x, t)\n",
    "        \n",
    "        alpha = alphas[t_inv]\n",
    "        alpha_hat = alphas_cumprod[t_inv]\n",
    "        beta = betas[t_inv]\n",
    "        \n",
    "        if t_inv > 0:\n",
    "            noise = torch.randn_like(x)\n",
    "        else:\n",
    "            noise = 0\n",
    "            \n",
    "        x = (1 / torch.sqrt(alpha)) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * pred_noise) + torch.sqrt(beta) * noise\n",
    "    return x\n",
    "\n",
    "# ==========================================\n",
    "# 4. TRENING \n",
    "# ==========================================\n",
    "\n",
    "diff_model = DenoiseCNN_withSinusoidalPosEmb().to(DEVICE)\n",
    "dae_model = DenoisingAutoencoder().to(DEVICE)\n",
    "\n",
    "opt_diff = optim.Adam(diff_model.parameters(), lr=LR)\n",
    "opt_dae = optim.Adam(dae_model.parameters(), lr=LR)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(\"\\n--- Rozpoczynanie treningu (Dyfuzja + Autoencoder) ---\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    loss_d, loss_a = 0, 0\n",
    "    for x0 in dataloader:\n",
    "        x0 = x0.to(DEVICE)\n",
    "        \n",
    "        # 1. Trenowanie Dyfuzji\n",
    "        t = torch.randint(0, T_STEPS, (x0.shape[0],), device=DEVICE).long()\n",
    "        xt, noise = forward_diffusion_sample(x0, t)\n",
    "        \n",
    "        opt_diff.zero_grad()\n",
    "        pred_noise = diff_model(xt, t)\n",
    "        l_diff = criterion(pred_noise, noise)\n",
    "        l_diff.backward()\n",
    "        opt_diff.step()\n",
    "        loss_d += l_diff.item()\n",
    "        \n",
    "        # 2. Trenowanie Autoencodera\n",
    "        # DAE dostaje wejście zaszumione stałym poziomem szumu\n",
    "        dae_noise = torch.randn_like(x0) * 0.3\n",
    "        x_noisy_dae = x0 + dae_noise\n",
    "        \n",
    "        opt_dae.zero_grad()\n",
    "        x_rec = dae_model(x_noisy_dae)\n",
    "        l_dae = criterion(x_rec, x0)\n",
    "        l_dae.backward()\n",
    "        opt_dae.step()\n",
    "        loss_a += l_dae.item()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Diff Loss: {loss_d/len(dataloader):.4f} | DAE Loss: {loss_a/len(dataloader):.4f}\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. PORÓWNANIE I WYNIKI\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n--- Generowanie wyników porównawczych ---\")\n",
    "\n",
    "# Dane testowe\n",
    "test_y, _ = generate_random_curves(1, SEQ_LENGTH)\n",
    "x_test = torch.tensor(test_y).unsqueeze(1).to(DEVICE)\n",
    "\n",
    "# Dodanie szumu do testu\n",
    "noise_level = 0.3\n",
    "input_noise = torch.randn_like(x_test) * noise_level\n",
    "x_noisy = x_test + input_noise\n",
    "\n",
    "# Konwersja do numpy dla metod klasycznych i wykresów\n",
    "y_clean = x_test.squeeze().cpu().numpy()\n",
    "y_noisy = x_noisy.squeeze().cpu().numpy()\n",
    "\n",
    "# 1. Metoda: Savitzky-Golay (Klasyczna)\n",
    "start = time.time()\n",
    "y_sg = savgol_filter(y_noisy, window_length=15, polyorder=3)\n",
    "time_sg = time.time() - start\n",
    "\n",
    "# 2. Metoda: Denoising Autoencoder (DL Baseline)\n",
    "dae_model.eval()\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    y_dae = dae_model(x_noisy).squeeze().cpu().numpy()\n",
    "time_dae = time.time() - start\n",
    "\n",
    "# 3. Metoda: Twoja Dyfuzja\n",
    "diff_model.eval()\n",
    "start = time.time()\n",
    "\n",
    "# Traktujemy x_noisy jako punkt startowy w procesie dyfuzji (x_T).\n",
    "x_diff_out = sample_reverse_process(diff_model, x_noisy) \n",
    "time_diff = time.time() - start\n",
    "y_diff = x_diff_out.squeeze().cpu().numpy()\n",
    "\n",
    "# Obliczenie metryk\n",
    "results = {\n",
    "    \"Metoda\": [\"Zaszumiony (Brak)\", \"Savitzky-Golay\", \"Autoencoder (DAE)\", \"Twój Model (Dyfuzja)\"],\n",
    "    \"MSE\": [\n",
    "        mean_squared_error(y_clean, y_noisy),\n",
    "        mean_squared_error(y_clean, y_sg),\n",
    "        mean_squared_error(y_clean, y_dae),\n",
    "        mean_squared_error(y_clean, y_diff)\n",
    "    ],\n",
    "    \"Czas [s]\": [0, time_sg, time_dae, time_diff]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\nTABELA WYNIKÓW:\")\n",
    "print(df)\n",
    "\n",
    "# Wykres\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(x_axis, y_clean, 'k-', linewidth=3, label=\"Oryginał (Ground Truth)\", alpha=0.5)\n",
    "plt.plot(x_axis, y_noisy, 'k.', label=\"Zaszumiony\", alpha=0.3)\n",
    "\n",
    "plt.plot(x_axis, y_sg, 'g--', linewidth=2, label=f\"Savitzky-Golay (MSE={df.iloc[1]['MSE']:.4f})\")\n",
    "plt.plot(x_axis, y_dae, 'b--', linewidth=2, label=f\"Autoencoder (MSE={df.iloc[2]['MSE']:.4f})\")\n",
    "plt.plot(x_axis, y_diff, 'r-', linewidth=2, label=f\"Dyfuzja (MSE={df.iloc[3]['MSE']:.4f})\")\n",
    "\n",
    "plt.title(\"Porównanie modelu dyfuzyjnego Denoise z istniejącymi narzędziami\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
